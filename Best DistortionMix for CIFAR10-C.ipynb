{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f467fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T05:17:08.497251Z",
     "iopub.status.busy": "2025-05-25T05:17:08.497018Z",
     "iopub.status.idle": "2025-05-25T05:18:31.084182Z",
     "shell.execute_reply": "2025-05-25T05:18:31.083018Z"
    },
    "executionInfo": {
     "elapsed": 80563,
     "status": "ok",
     "timestamp": 1748175097173,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "f0f467fc",
    "outputId": "7622e006-7142-4f3b-cae8-7471e22c88f6",
    "papermill": {
     "duration": 82.594921,
     "end_time": "2025-05-25T05:18:31.085553",
     "exception": false,
     "start_time": "2025-05-25T05:17:08.490632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/RobustBench/robustbench.git\n",
    "!git clone https://github.com/kentaroy47/vision-transformers-cifar10.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UVRvMswTCqiW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5513,
     "status": "ok",
     "timestamp": 1748197686853,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "UVRvMswTCqiW",
    "outputId": "0b5de246-e5eb-4e3e-eded-e3aa6182738f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)\n",
    "\n",
    "# Define project path\n",
    "drive_path = '/content/drive/My Drive/Paper/Robustness Image Classification/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520bf0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:18:31.148620Z",
     "iopub.status.busy": "2025-05-25T05:18:31.147770Z",
     "iopub.status.idle": "2025-05-25T05:18:31.157413Z",
     "shell.execute_reply": "2025-05-25T05:18:31.156725Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1748197726994,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "5520bf0d",
    "papermill": {
     "duration": 0.039486,
     "end_time": "2025-05-25T05:18:31.158611",
     "exception": false,
     "start_time": "2025-05-25T05:18:31.119125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "# EPOCHS = 50\n",
    "# LR = 0.01\n",
    "# NUM_CLASSES = 10\n",
    "# BATCH_SIZE = 32\n",
    "# IMG_SIZE = 224\n",
    "EPOCHS = 20\n",
    "LR = 1e-4\n",
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 100\n",
    "IMG_SIZE = 384\n",
    "size = 384\n",
    "patch = 4\n",
    "dimhead = 512\n",
    "convkernel = 8\n",
    "ROOT_DATA = './data'\n",
    "CIFAR10_CORRUPT_PATH = f'{drive_path}Dataset/CIFAR-10-C'\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "corruption_types = [\n",
    "    'brightness', 'contrast', 'defocus_blur', 'elastic_transform',\n",
    "    'fog', 'frost', 'gaussian_blur', 'gaussian_noise',\n",
    "    'glass_blur', 'impulse_noise', 'jpeg_compression', 'motion_blur',\n",
    "    'pixelate', 'saturate', 'shot_noise', 'snow', 'spatter',\n",
    "    'speckle_noise', 'zoom_blur'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf17e025",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:18:31.226305Z",
     "iopub.status.busy": "2025-05-25T05:18:31.226093Z",
     "iopub.status.idle": "2025-05-25T05:18:45.635630Z",
     "shell.execute_reply": "2025-05-25T05:18:45.635076Z"
    },
    "executionInfo": {
     "elapsed": 12614,
     "status": "ok",
     "timestamp": 1748176319521,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "bf17e025",
    "papermill": {
     "duration": 14.438412,
     "end_time": "2025-05-25T05:18:45.637043",
     "exception": false,
     "start_time": "2025-05-25T05:18:31.198631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TrF\n",
    "import argparse, math, os, random, sys, torch, torchvision, time, timm\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "from robustbench.data import load_cifar10c\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, SubsetRandomSampler\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/vision-transformers-cifar10')\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66f0c8",
   "metadata": {
    "id": "7b66f0c8",
    "papermill": {
     "duration": 0.025922,
     "end_time": "2025-05-25T05:18:45.690116",
     "exception": false,
     "start_time": "2025-05-25T05:18:45.664194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Read & Plot Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7accc4f8",
   "metadata": {
    "id": "7accc4f8",
    "papermill": {
     "duration": 0.06991,
     "end_time": "2025-05-25T05:18:45.786697",
     "exception": false,
     "start_time": "2025-05-25T05:18:45.716787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. Implementasi Data/Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61abbfd0",
   "metadata": {
    "id": "61abbfd0",
    "papermill": {
     "duration": 0.025987,
     "end_time": "2025-05-25T05:18:45.838469",
     "exception": false,
     "start_time": "2025-05-25T05:18:45.812482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Dataset CIFAR-10 merupakan dataset yang dibentuk oleh tim riset University of Toronto https://www.cs.toronto.edu/~kriz/cifar.html, terdiri dari 60,000 gambar RGB (50,000 train dan 10,000 test) berdimensi 32x32. CIFAR-10 memiliki 10 kelas / kategori objek: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.\n",
    "\n",
    "a. Tipe data yang digunakan\n",
    "\n",
    "Array numpy yang berisi data gambar mentah yang jika dilihat berukuran 32x32, selanjutnya dilakukan preprocessing sehingga didapatkan:\n",
    "\n",
    "trainloader, testloader\n",
    "\n",
    "b. Pra-pemrosesan data yang dilakukan\n",
    "\n",
    "Dataset yang diload dari PyTorch secara langsung lalu diolah dan ditransformasi. Transformasi yang didefinisikan meliputi proses seperti pemangkasan acak (random crop) dengan padding 4 piksel, flipping horizontal secara acak, dan mengubah data menjadi tensor (untuk data train dan testing). Ukuran batch pada train yaitu 128, serta 100 pada data testing. Data train dilakukan shuffle, sedangkan data testing tidak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfeea9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:18:45.890935Z",
     "iopub.status.busy": "2025-05-25T05:18:45.890443Z",
     "iopub.status.idle": "2025-05-25T05:18:45.899201Z",
     "shell.execute_reply": "2025-05-25T05:18:45.898666Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1748176319527,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "9dfeea9a",
    "papermill": {
     "duration": 0.036091,
     "end_time": "2025-05-25T05:18:45.900186",
     "exception": false,
     "start_time": "2025-05-25T05:18:45.864095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomImageTransform(object):\n",
    "    def __init__(self, impulse_prob=0.1, gauss_prob=0.1, contrast_prob=0.1, fog_prob=0.1, noise_prob=0.01, amplitude=10000, noise_std=0.05, contrast_range=(0.5, 1.5), fog_density=0.5):\n",
    "        self.impulse_prob = impulse_prob\n",
    "        self.gauss_prob = gauss_prob\n",
    "        self.contrast_prob = contrast_prob\n",
    "        self.fog_prob = fog_prob\n",
    "        self.noise_prob = noise_prob\n",
    "        self.amplitude = amplitude\n",
    "        self.noise_std = noise_std\n",
    "        self.contrast_range = contrast_range\n",
    "        self.fog_density = fog_density\n",
    "\n",
    "    def __call__(self, img):\n",
    "        transform_prob = random.random()\n",
    "        if transform_prob < self.contrast_prob:\n",
    "            img = self.apply_contrast(img)\n",
    "        elif transform_prob < self.contrast_prob + self.gauss_prob:\n",
    "            img = self.apply_gaussian_noise(img)\n",
    "        elif transform_prob < self.contrast_prob + self.gauss_prob + self.impulse_prob:\n",
    "            img = self.apply_impulse_noise(img)\n",
    "#         elif transform_prob < self.contrast_prob + self.gauss_prob + self.impulse_prob + self.fog_prob:\n",
    "#             img = self.apply_fog(img)\n",
    "        return img\n",
    "\n",
    "    def apply_impulse_noise(self, img):\n",
    "        noise_mask = torch.rand_like(img)\n",
    "        salt = (noise_mask < self.noise_prob / 2).float()\n",
    "        pepper = (noise_mask > 1 - self.noise_prob / 2).float()\n",
    "\n",
    "        noisy_img = img.clone()\n",
    "        noisy_img += salt * 255.0 * 2 * self.amplitude\n",
    "        noisy_img -= pepper * 255.0 * 2 * self.amplitude\n",
    "        noisy_img = torch.clamp(noisy_img, 0, 255)\n",
    "        return noisy_img\n",
    "\n",
    "    def apply_gaussian_noise(self, img):\n",
    "        noise = torch.randn_like(img) * self.noise_std\n",
    "        noisy_img = img + noise\n",
    "        noisy_img = torch.clamp(noisy_img, 0, 1)\n",
    "        return noisy_img\n",
    "\n",
    "    def apply_contrast(self, img):\n",
    "        factor = random.uniform(*self.contrast_range)\n",
    "        mean = torch.mean(img)\n",
    "        img = (img - mean) * factor + mean\n",
    "        img = torch.clamp(img, 0, 255)\n",
    "        return img\n",
    "\n",
    "    def apply_fog(self, img):\n",
    "        fog_density = torch.randn_like(img) * self.noise_std + self.fog_density\n",
    "        fog_density = torch.clamp(fog_density, 0, 1)  # Ensure values are between 0 and 1\n",
    "\n",
    "        # Add fog to the image\n",
    "        img = img * (1 - fog_density) + fog_density\n",
    "        img = torch.clamp(img, 0, 1)  # Ensure values are between 0 and 1\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa20bfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T05:18:45.952223Z",
     "iopub.status.busy": "2025-05-25T05:18:45.952024Z",
     "iopub.status.idle": "2025-05-25T05:19:02.728766Z",
     "shell.execute_reply": "2025-05-25T05:19:02.728183Z"
    },
    "executionInfo": {
     "elapsed": 18507,
     "status": "ok",
     "timestamp": 1748176338049,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "3aa20bfd",
    "outputId": "97009d60-8e6c-42b9-b7cb-eca5e181f714",
    "papermill": {
     "duration": 16.804081,
     "end_time": "2025-05-25T05:19:02.730123",
     "exception": false,
     "start_time": "2025-05-25T05:18:45.926042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Melakukan pembacaan data dari sumber, serta melakukan preprocessing\n",
    "print('==> Preparing data..')\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    RandomImageTransform(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=ROOT_DATA, train=True, download=True, transform=transform_train)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=ROOT_DATA, train=False, download=True, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc2d1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:02.787595Z",
     "iopub.status.busy": "2025-05-25T05:19:02.787380Z",
     "iopub.status.idle": "2025-05-25T05:19:02.791131Z",
     "shell.execute_reply": "2025-05-25T05:19:02.790544Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1748176338068,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "27bc2d1b",
    "outputId": "c1c4cad6-c707-4a3d-ecdb-9e3cb3a5902f",
    "papermill": {
     "duration": 0.033198,
     "end_time": "2025-05-25T05:19:02.792201",
     "exception": false,
     "start_time": "2025-05-25T05:19:02.759003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Train set size:\", len(trainloader.dataset))\n",
    "print(\"Test set size:\", len(testloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d378f28f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1rHsr3oq0K_wD5Nn_FJCNcsh57tRZe_Ah"
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:02.849167Z",
     "iopub.status.busy": "2025-05-25T05:19:02.848735Z",
     "iopub.status.idle": "2025-05-25T05:19:07.978030Z",
     "shell.execute_reply": "2025-05-25T05:19:07.977365Z"
    },
    "executionInfo": {
     "elapsed": 8193,
     "status": "ok",
     "timestamp": 1748176346264,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "d378f28f",
    "outputId": "d14591a4-b220-40be-81e9-5f4ec451f135",
    "papermill": {
     "duration": 5.179884,
     "end_time": "2025-05-25T05:19:08.000408",
     "exception": false,
     "start_time": "2025-05-25T05:19:02.820524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot gambar per kelas\n",
    "def plot_cifar10(cifar_dataset, data_title, num_images_per_class=5):\n",
    "    classes = cifar_dataset.classes\n",
    "    class_indices = {class_name: [] for class_name in classes}\n",
    "\n",
    "    # Memuat indeks sampel untuk setiap kelas\n",
    "    for i, (_, label) in enumerate(cifar_dataset):\n",
    "        class_name = classes[label]\n",
    "        if len(class_indices[class_name]) < num_images_per_class:\n",
    "            class_indices[class_name].append(i)\n",
    "\n",
    "        # Keluar dari loop jika semua kelas sudah memiliki sampel yang cukup\n",
    "        if all(len(indices) == num_images_per_class for indices in class_indices.values()):\n",
    "            break\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=len(classes), ncols=num_images_per_class, figsize=(10, 20))\n",
    "\n",
    "    # Memuat dan menampilkan gambar\n",
    "    for i, class_name in enumerate(classes):\n",
    "        for j in range(num_images_per_class):\n",
    "            index = class_indices[class_name][j]\n",
    "            image, _ = cifar_dataset[index]\n",
    "            axes[i, j].imshow(image.permute(1, 2, 0))\n",
    "            axes[i, j].set_xticks([])\n",
    "            axes[i, j].set_yticks([])\n",
    "            axes[i, j].set_title(class_name)\n",
    "\n",
    "    plt.suptitle(f'Plot CIFAR10 | {num_images_per_class} Examples ({data_title})', y=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_cifar10(trainloader.dataset,'Train')\n",
    "plot_cifar10(testloader.dataset,'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f3cbc",
   "metadata": {
    "id": "4d9f3cbc",
    "papermill": {
     "duration": 0.073296,
     "end_time": "2025-05-25T05:19:08.147294",
     "exception": false,
     "start_time": "2025-05-25T05:19:08.073998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pembangunan Arsitektur Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0bd1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:08.398009Z",
     "iopub.status.busy": "2025-05-25T05:19:08.397707Z",
     "iopub.status.idle": "2025-05-25T05:19:08.401304Z",
     "shell.execute_reply": "2025-05-25T05:19:08.400752Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1748176346296,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "e0f0bd1d",
    "papermill": {
     "duration": 0.067768,
     "end_time": "2025-05-25T05:19:08.402323",
     "exception": false,
     "start_time": "2025-05-25T05:19:08.334555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pretrainedmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff3b383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:08.528227Z",
     "iopub.status.busy": "2025-05-25T05:19:08.527981Z",
     "iopub.status.idle": "2025-05-25T05:19:08.531921Z",
     "shell.execute_reply": "2025-05-25T05:19:08.531189Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1748176346301,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "5ff3b383",
    "papermill": {
     "duration": 0.068064,
     "end_time": "2025-05-25T05:19:08.532993",
     "exception": false,
     "start_time": "2025-05-25T05:19:08.464929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pretrainedmodels\n",
    "# import ssl\n",
    "# from timm import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1ba44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:08.661096Z",
     "iopub.status.busy": "2025-05-25T05:19:08.660814Z",
     "iopub.status.idle": "2025-05-25T05:19:08.664596Z",
     "shell.execute_reply": "2025-05-25T05:19:08.663960Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1748176346306,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "14b1ba44",
    "papermill": {
     "duration": 0.070024,
     "end_time": "2025-05-25T05:19:08.665775",
     "exception": false,
     "start_time": "2025-05-25T05:19:08.595751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def custom_model():\n",
    "#     # Define the resnet model\n",
    "# #     ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# #     model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "#     model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
    "# #     model = torchvision.models.efficientnetb0(weights='IMAGENET1K_V1')\n",
    "\n",
    "# #     for param in model.parameters():\n",
    "# #         param.requires_grad = False\n",
    "\n",
    "#     # Update the fully connected layer of resnet with our current target of 10 desired outputs\n",
    "#     model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "\n",
    "# #     model = create_model('efficientnet_b0', pretrained=True)\n",
    "# #     for param in model.parameters():\n",
    "# #         param.requires_grad = False\n",
    "#     # Make the parameters of certain layers trainable\n",
    "# #     for name, param in model.named_parameters():\n",
    "# #         if 'classifier' in name:  # Modify this condition as per your requirement\n",
    "# #             param.requires_grad = True  # Unfreeze the parameters of the classifier layer\n",
    "# #     model.classifier = nn.Linear(model.classifier.in_features, NUM_CLASSES)\n",
    "\n",
    "# #     model = pretrainedmodels.__dict__[\"inceptionresnetv2\"](pretrained='imagenet')\n",
    "# #     model.last_linear = nn.Linear(model.last_linear.in_features, NUM_CLASSES)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d05aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:08.794643Z",
     "iopub.status.busy": "2025-05-25T05:19:08.794395Z",
     "iopub.status.idle": "2025-05-25T05:19:08.812969Z",
     "shell.execute_reply": "2025-05-25T05:19:08.812401Z"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1748176346325,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "071d05aa",
    "papermill": {
     "duration": 0.085674,
     "end_time": "2025-05-25T05:19:08.814078",
     "exception": false,
     "start_time": "2025-05-25T05:19:08.728404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_model(model_name='resnet34'):\n",
    "    if model_name == 'resnet18':\n",
    "        model = torchvision.models.resnet18(pretrained=True)\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'resnet34':\n",
    "        model = torchvision.models.resnet34(weights='IMAGENET1K_V1')\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'resnet50':\n",
    "        model = torchvision.models.resnet50(weights='IMAGENET1K_V1')\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'resnet101':\n",
    "        model = torchvision.models.resnet101(weights='IMAGENET1K_V1')\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'resnet152':\n",
    "        model = torchvision.models.resnet152(weights='IMAGENET1K_V1')\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'wide_resnet50_2':\n",
    "        model = torchvision.models.wide_resnet50_2(pretrained=True)\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'wide_resnet101_2':\n",
    "        model = torchvision.models.wide_resnet101_2(weights='IMAGENET1K_V1')\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'mnasnet1_0':\n",
    "        model = torchvision.models.mnasnet1_0(pretrained=True)\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n",
    "    elif model_name == 'alexnet':\n",
    "        model = torchvision.models.alexnet(pretrained=True)\n",
    "        model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, NUM_CLASSES)\n",
    "    elif model_name == 'googlenet':\n",
    "        model = torchvision.models.googlenet(pretrained=True)\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'vgg16':\n",
    "        model = torchvision.models.vgg16(pretrained=True)\n",
    "        num_ftrs = model.classifier[6].in_features\n",
    "        model.classifier[6] = torch.nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "    elif model_name == 'inception_v3':\n",
    "        model = torchvision.models.inception_v3(pretrained=True)\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'mobilenetv2':\n",
    "        model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n",
    "    elif model_name == 'mobilenetv3_small':\n",
    "        model = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
    "        print(model.classifier)\n",
    "        model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, NUM_CLASSES)\n",
    "    elif model_name == 'mobilenetv3_large':\n",
    "        model = torchvision.models.mobilenet_v3_large(pretrained=True)\n",
    "        model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, NUM_CLASSES)\n",
    "    elif model_name == 'densenet121':\n",
    "        model = torchvision.models.densenet121(pretrained=True)\n",
    "        model.classifier = torch.nn.Linear(model.classifier.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'densenet161':\n",
    "        model = torchvision.models.densenet161(pretrained=True)\n",
    "        model.classifier = torch.nn.Linear(model.classifier.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'densenet169':\n",
    "        model = torchvision.models.densenet169(weights='IMAGENET1K_V1')\n",
    "        model.classifier = torch.nn.Linear(model.classifier.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'densenet201':\n",
    "        model = torchvision.models.densenet201(weights='IMAGENET1K_V1')\n",
    "        model.classifier = torch.nn.Linear(model.classifier.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'squeezenet1_0':\n",
    "        model = torchvision.models.squeezenet1_0(pretrained=True)\n",
    "        model.classifier[1] = torch.nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n",
    "    elif model_name == 'squeezenet1_1':\n",
    "        model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "        model.classifier[1] = torch.nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n",
    "    elif model_name == 'resnext50_32x4d':\n",
    "        model = torchvision.models.resnext50_32x4d(pretrained=True)\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'shufflenet_v2_x1_0':\n",
    "        model = torchvision.models.shufflenet_v2_x1_0(pretrained=True)\n",
    "        model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'vit_b_16':\n",
    "        model = torchvision.models.vit_b_16(weights='IMAGENET1K_V1')\n",
    "        model.heads.head = torch.nn.Linear(model.heads.head.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'vit_b_32':\n",
    "        model = torchvision.models.vit_b_32(weights='IMAGENET1K_V1')\n",
    "        model.heads.head = torch.nn.Linear(model.heads.head.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'vit_h_14':\n",
    "        model = torchvision.models.vit_h_14()\n",
    "        model.heads.head = torch.nn.Linear(model.heads.head.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'vit_l_16':\n",
    "        model = torchvision.models.vit_l_16(weights='IMAGENET1K_V1')\n",
    "        model.heads.head = torch.nn.Linear(model.heads.head.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'vit_l_32':\n",
    "        model = torchvision.models.vit_l_32(weights='IMAGENET1K_V1')\n",
    "        model.heads.head = torch.nn.Linear(model.heads.head.in_features, NUM_CLASSES)\n",
    "    elif model_name == \"vit_timm\":\n",
    "        model = timm.create_model(\"vit_base_patch16_384\", pretrained=True)\n",
    "        model.head = nn.Linear(model.head.in_features, NUM_CLASSES)\n",
    "    elif model_name == 'efficientnet_b0':\n",
    "        model = torchvision.models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n",
    "    elif model_name == 'efficientnet_b1':\n",
    "        model = torchvision.models.efficientnet_b1(weights='IMAGENET1K_V1')\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n",
    "    elif model_name == 'efficientnet_b2':\n",
    "        model = torchvision.models.efficientnet_b2(weights='IMAGENET1K_V1')\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n",
    "    elif model_name == 'efficientnet_b3':\n",
    "        model = torchvision.models.efficientnet_b3(weights='IMAGENET1K_V1')\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n",
    "    elif model_name == 'efficientnet_b4':\n",
    "        model = torchvision.models.efficientnet_b4(weights='IMAGENET1K_V1')\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n",
    "    elif model_name == 'efficientnet_b5':\n",
    "        model = torchvision.models.efficientnet_b5(weights='IMAGENET1K_V1')\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n",
    "    elif model_name == 'efficientnet_b6':\n",
    "        model = torchvision.models.efficientnet_b6(weights='IMAGENET1K_V1')\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n",
    "    elif model_name == 'efficientnet_b7':\n",
    "        model = torchvision.models.efficientnet_b7(weights='IMAGENET1K_V1')\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, NUM_CLASSES)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name! Supported models: vgg, inception, mobilenet, densenet, squeezenet, resnext, nasnet, xception, shufflenet\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97289203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:08.940670Z",
     "iopub.status.busy": "2025-05-25T05:19:08.940421Z",
     "iopub.status.idle": "2025-05-25T05:19:08.944052Z",
     "shell.execute_reply": "2025-05-25T05:19:08.943421Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748176346330,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "97289203",
    "papermill": {
     "duration": 0.067682,
     "end_time": "2025-05-25T05:19:08.945134",
     "exception": false,
     "start_time": "2025-05-25T05:19:08.877452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install einops\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "# from typing import Callable\n",
    "# from einops import rearrange, reduce, repeat\n",
    "\n",
    "# class ConvTokenizer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         channels: int = 3, emb_dim: int = 256,\n",
    "#         conv_kernel: int = 3, conv_stride: int = 2, conv_pad: int = 3,\n",
    "#         pool_kernel: int = 3, pool_stride: int = 2, pool_pad: int = 1,\n",
    "#         activation: Callable = nn.ReLU\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Conv2d(\n",
    "#             in_channels=channels, out_channels=emb_dim,\n",
    "#             kernel_size=conv_kernel, stride=conv_stride,\n",
    "#             padding=(conv_pad, conv_pad)\n",
    "#         )\n",
    "#         self.act = activation(inplace=True)\n",
    "#         self.max_pool = nn.MaxPool2d(\n",
    "#             kernel_size=pool_kernel, stride=pool_stride,\n",
    "#             padding=pool_pad\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.act(x)\n",
    "#         x = self.max_pool(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8285d97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:09.070608Z",
     "iopub.status.busy": "2025-05-25T05:19:09.070322Z",
     "iopub.status.idle": "2025-05-25T05:19:09.073756Z",
     "shell.execute_reply": "2025-05-25T05:19:09.073295Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748176346337,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "b8285d97",
    "papermill": {
     "duration": 0.067234,
     "end_time": "2025-05-25T05:19:09.074778",
     "exception": false,
     "start_time": "2025-05-25T05:19:09.007544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class TransformerEncoderBlock(nn.Module):\n",
    "#     \"\"\"Transformer Encoder Block\"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self, n_h: int, emb_dim: int, feat_dim: int,\n",
    "#         dropout: float = 0, attention_dropout: float = 0\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.msa = MSA(heads=n_h, emb_dim=emb_dim, dropout=dropout, attention_dropout=attention_dropout)\n",
    "#         self.norm1 = nn.LayerNorm(emb_dim)\n",
    "#         self.ffn = MLP(emb_dim, feat_dim, dropout)\n",
    "#         self.norm2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         identity = x\n",
    "#         x = self.msa(x)\n",
    "#         x += identity\n",
    "#         x = self.norm1(x)\n",
    "#         identity = x\n",
    "#         x = self.ffn(x)\n",
    "#         x += identity\n",
    "#         x = self.norm2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea4b61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:09.202080Z",
     "iopub.status.busy": "2025-05-25T05:19:09.201429Z",
     "iopub.status.idle": "2025-05-25T05:19:09.206239Z",
     "shell.execute_reply": "2025-05-25T05:19:09.205718Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748176346342,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "66ea4b61",
    "papermill": {
     "duration": 0.069314,
     "end_time": "2025-05-25T05:19:09.207317",
     "exception": false,
     "start_time": "2025-05-25T05:19:09.138003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class MSA(nn.Module):\n",
    "#     \"\"\"Multi-head Self Attention Block\"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self, heads: int, emb_dim: int,\n",
    "#         dropout: float = 0., attention_dropout: float = 0.\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.emb_dim = emb_dim\n",
    "#         self.n_h = heads\n",
    "#         self.head_dim = self.emb_dim // self.n_h\n",
    "#         self.q = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "#         self.k = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "#         self.v = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "#         self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "#         self.linear_projection = nn.Linear(self.emb_dim, self.emb_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         # (bs,     s_l,      e_d)\n",
    "#         batch_s, seq_len, emb_dim = x.shape\n",
    "#         # (bs, s_l, e_d) -> (bs, s_l, n_h, h_d) -> (bs, n_h, s_l, h_d)\n",
    "#         x_q = self.q(x).view(\n",
    "#             batch_s, seq_len, self.n_h, self.head_dim).transpose(1, 2)\n",
    "#         x_k = self.k(x).view(\n",
    "#             batch_s, seq_len, self.n_h, self.head_dim).transpose(1, 2)\n",
    "#         x_v = self.v(x).view(\n",
    "#             batch_s, seq_len, self.n_h, self.head_dim).transpose(1, 2)\n",
    "#         # @ operator is the convention for matrix multiplication, throughout python\n",
    "#         # q @ k.T -> (bs, n_h, s_l, h_d) @ (bs, n_h, h_d, s_l) -> (bs, n_h, s_l, s_l)\n",
    "#         # Softmax((q @ k.T)/root(h_d)) @ v\n",
    "#         #   -> (bs, n_h, s_l, s_l) @ (bs, n_h, s_l, h_d) -> (bs, n_h, s_l, h_d)\n",
    "#         attention = (x_q @ x_k.transpose(-2, -1)) / math.sqrt(x_q.size(-1))\n",
    "#         attention = F.softmax(attention, dim=-1)\n",
    "#         attention = self.attention_dropout(attention)\n",
    "#         # (bs, n_h, s_l, h_d) -> (bs, s_l, n_h, h_d) -> (bs, s_l, e_d)\n",
    "#         x = (attention @ x_v).transpose(1, 2).reshape(batch_s, seq_len, emb_dim)\n",
    "#         x = self.linear_projection(x)\n",
    "#         x = self.dropout(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3b21f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:09.333954Z",
     "iopub.status.busy": "2025-05-25T05:19:09.333697Z",
     "iopub.status.idle": "2025-05-25T05:19:09.337171Z",
     "shell.execute_reply": "2025-05-25T05:19:09.336671Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1748176346358,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "9ec3b21f",
    "papermill": {
     "duration": 0.067272,
     "end_time": "2025-05-25T05:19:09.338161",
     "exception": false,
     "start_time": "2025-05-25T05:19:09.270889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     \"\"\"MLP block\"\"\"\n",
    "\n",
    "#     def __init__(self, emb_dim: int, feat_dim: int, dropout: float = 0):\n",
    "#         super().__init__()\n",
    "#         self.layer1 = nn.Linear(emb_dim, feat_dim)\n",
    "#         self.activation = nn.GELU()\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.layer2 = nn.Linear(feat_dim, emb_dim)\n",
    "\n",
    "#         # below init from torchvision\n",
    "#         nn.init.xavier_uniform_(self.layer1.weight)\n",
    "#         nn.init.xavier_uniform_(self.layer2.weight)\n",
    "#         nn.init.normal_(self.layer1.bias, std=1e-6)\n",
    "#         nn.init.normal_(self.layer2.bias, std=1e-6)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.activation(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.dropout(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667bef3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:09.469797Z",
     "iopub.status.busy": "2025-05-25T05:19:09.469566Z",
     "iopub.status.idle": "2025-05-25T05:19:09.472894Z",
     "shell.execute_reply": "2025-05-25T05:19:09.472343Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748176346363,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "0667bef3",
    "papermill": {
     "duration": 0.069579,
     "end_time": "2025-05-25T05:19:09.473999",
     "exception": false,
     "start_time": "2025-05-25T05:19:09.404420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class SeqPool(nn.Module):\n",
    "#     def __init__(self, emb_dim=256):\n",
    "#         super().__init__()\n",
    "#         self.dense = nn.Linear(emb_dim, 1)\n",
    "#         self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         bs, seq_len, emb_dim = x.shape\n",
    "#         identity = x\n",
    "#         x = self.dense(x)\n",
    "#         x = rearrange(\n",
    "#             x, 'bs seq_len 1 -> bs 1 seq_len', seq_len=seq_len\n",
    "#         )\n",
    "#         x = self.softmax(x)\n",
    "#         x = x @ identity\n",
    "#         x = rearrange(\n",
    "#             x, 'bs 1 e_d -> bs e_d', e_d=emb_dim\n",
    "#         )\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d783d0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:09.607596Z",
     "iopub.status.busy": "2025-05-25T05:19:09.607357Z",
     "iopub.status.idle": "2025-05-25T05:19:09.611410Z",
     "shell.execute_reply": "2025-05-25T05:19:09.610897Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748176346368,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "8d783d0c",
    "papermill": {
     "duration": 0.070087,
     "end_time": "2025-05-25T05:19:09.612414",
     "exception": false,
     "start_time": "2025-05-25T05:19:09.542327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CCT(nn.Module):\n",
    "#     \"\"\"\n",
    "#         Compact Convolutional Transformer (CCT) Model\n",
    "#         https://arxiv.org/abs/2104.05704v4\n",
    "#     \"\"\"\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         conv_kernel: int = 3, conv_stride: int = 2, conv_pad: int = 3,\n",
    "#         pool_kernel: int = 3, pool_stride: int = 2, pool_pad: int = 1,\n",
    "#         heads: int = 4, emb_dim: int = 256, feat_dim: int = 2*256,\n",
    "#         dropout: float = 0.1, attention_dropout: float = 0.1, layers: int = 7,\n",
    "#         channels: int = 3, image_size: int = 32, num_class: int = 10\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.emb_dim = emb_dim\n",
    "#         self.image_size = image_size\n",
    "\n",
    "#         self.tokenizer = ConvTokenizer(\n",
    "#             channels=channels, emb_dim=self.emb_dim,\n",
    "#             conv_kernel=conv_kernel, conv_stride=conv_stride, conv_pad=conv_pad,\n",
    "#             pool_kernel=pool_kernel, pool_stride=pool_stride, pool_pad=pool_pad,\n",
    "#             activation=nn.ReLU\n",
    "#         )\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             x = torch.randn([1, channels, image_size, image_size])\n",
    "#             out = self.tokenizer(x)\n",
    "#             _, _, ph_c, pw_c  = out.shape\n",
    "\n",
    "#         self.linear_projection = nn.Linear(\n",
    "#             ph_c, pw_c, self.emb_dim\n",
    "#         )\n",
    "\n",
    "#         self.pos_emb = nn.Parameter(\n",
    "#             torch.randn(\n",
    "#                 [1, ph_c*pw_c, self.emb_dim]\n",
    "#             ).normal_(std=0.02) # from torchvision, which takes this from BERT\n",
    "#         )\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         encoders = []\n",
    "#         for _ in range(0, layers):\n",
    "#             encoders.append(\n",
    "#                 TransformerEncoderBlock(\n",
    "#                     n_h=heads, emb_dim=self.emb_dim, feat_dim=feat_dim,\n",
    "#                     dropout=dropout, attention_dropout=attention_dropout\n",
    "#                 )\n",
    "#             )\n",
    "#         self.encoder_stack = nn.Sequential(*encoders)\n",
    "#         self.seq_pool = SeqPool(emb_dim=self.emb_dim)\n",
    "#         self.mlp_head = nn.Linear(self.emb_dim, num_class)\n",
    "\n",
    "\n",
    "#     def forward(self, x: torch.Tensor):\n",
    "#         bs, c, h, w = x.shape  # (bs, c, h, w)\n",
    "\n",
    "#         # Creates overlapping patches using ConvNet\n",
    "#         x = self.tokenizer(x)\n",
    "#         x = rearrange(\n",
    "#             x, 'bs e_d ph_h ph_w -> bs (ph_h ph_w) e_d',\n",
    "#             bs=bs, e_d=self.emb_dim\n",
    "#         )\n",
    "\n",
    "#         # Add position embedding\n",
    "#         x = self.pos_emb.expand(bs, -1, -1) + x\n",
    "#         x = self.dropout(x)\n",
    "\n",
    "#         # Pass through Transformer Encoder layers\n",
    "#         x = self.encoder_stack(x)\n",
    "\n",
    "#         # Perform Sequential Pooling <- Novelty of the paper\n",
    "#         x = self.seq_pool(x)\n",
    "\n",
    "#         # MLP head used to get logits\n",
    "#         x = self.mlp_head(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62b4e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:09.738831Z",
     "iopub.status.busy": "2025-05-25T05:19:09.738168Z",
     "iopub.status.idle": "2025-05-25T05:19:09.742360Z",
     "shell.execute_reply": "2025-05-25T05:19:09.741832Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1748176346375,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "0d62b4e1",
    "papermill": {
     "duration": 0.068541,
     "end_time": "2025-05-25T05:19:09.743415",
     "exception": false,
     "start_time": "2025-05-25T05:19:09.674874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def custom_model():\n",
    "#     config = dict(\n",
    "#         seed=42, # <-- the answer to life, the universe and everything\n",
    "#         heads=4, emb_dim=256, feat_dim=512, layers=7,\n",
    "#         num_class=NUM_CLASSES, image_size=IMG_SIZE,\n",
    "#         dropout=0.5, attention_dropout=0.1,\n",
    "#     )\n",
    "\n",
    "#     model = CCT(\n",
    "#         heads=config['heads'], emb_dim=config['emb_dim'],\n",
    "#         feat_dim=config['feat_dim'], layers=config['layers'],\n",
    "#         num_class=config['num_class'], image_size=config['image_size'],\n",
    "#         dropout=config['dropout'], attention_dropout=config['attention_dropout']\n",
    "#     )\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4f0e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:09.870037Z",
     "iopub.status.busy": "2025-05-25T05:19:09.869760Z",
     "iopub.status.idle": "2025-05-25T05:19:09.873382Z",
     "shell.execute_reply": "2025-05-25T05:19:09.872826Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1748176346383,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "f4d4f0e0",
    "papermill": {
     "duration": 0.068704,
     "end_time": "2025-05-25T05:19:09.874431",
     "exception": false,
     "start_time": "2025-05-25T05:19:09.805727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         # Convolutional layers\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "#         # Max pooling layers\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         # Fully connected layers\n",
    "# #         self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "#         self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "#         self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(F.relu(self.conv3(x)))\n",
    "#         x = x.view(-1, 128 * 4 * 4)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# def custom_model():\n",
    "#     return CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48398862",
   "metadata": {
    "id": "48398862",
    "papermill": {
     "duration": 0.062179,
     "end_time": "2025-05-25T05:19:09.999607",
     "exception": false,
     "start_time": "2025-05-25T05:19:09.937428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Secara ringkas, optimizer SGD digunakan untuk mengoptimalkan parameter-parameter model dengan mengurangi fungsi kerugian (loss function) selama pelatihan. Momentum dan weight decay digunakan untuk membantu percepatan konvergensi dan mencegah overfitting. Sementara itu, penjadwalan laju pembelajaran dengan penurunan kosinus memastikan bahwa laju pembelajaran dikurangi secara bertahap selama pelatihan, sehingga memungkinkan model untuk menemukan minimum global secara lebih stabil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb05ecf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178,
     "referenced_widgets": [
      "9cd21ab96eef4b54b4a1acc0938e8150",
      "0158009f6d4d495a82a208cd60737590",
      "77544320956d43c98a3945dbdde234b3",
      "79e536af7f3f447e8fb86630e08cd24e",
      "88c365a17851409eaff0b8ddf582b45f",
      "33bba534724e4342b298859d766d1b6a",
      "6f0af9e8a2794d2ebfb7fc00825b7cdc",
      "4d22d8f72880429eab807577d3510b6f",
      "71817e0194f14378b533cbb1c72c2987",
      "bc0a6b6b103e4a3e9f66dad6e165a86f",
      "addb4c4b2679449596aa2eb4a38dcf1f"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:10.125838Z",
     "iopub.status.busy": "2025-05-25T05:19:10.125085Z",
     "iopub.status.idle": "2025-05-25T05:19:11.048139Z",
     "shell.execute_reply": "2025-05-25T05:19:11.047397Z"
    },
    "executionInfo": {
     "elapsed": 1842,
     "status": "ok",
     "timestamp": 1748176348227,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "deb05ecf",
    "outputId": "f4c81cb8-8dc9-431d-e4cb-cdbd0759b197",
    "papermill": {
     "duration": 0.987598,
     "end_time": "2025-05-25T05:19:11.049410",
     "exception": false,
     "start_time": "2025-05-25T05:19:10.061812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = custom_model('vit_timm')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "n_epoch = EPOCHS\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e23acd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:11.439916Z",
     "iopub.status.busy": "2025-05-25T05:19:11.439611Z",
     "iopub.status.idle": "2025-05-25T05:19:11.444887Z",
     "shell.execute_reply": "2025-05-25T05:19:11.444145Z"
    },
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1748176348328,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "c7e23acd",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ba43f2c8-ef04-417f-a944-191f81fe13d4",
    "papermill": {
     "duration": 0.071822,
     "end_time": "2025-05-25T05:19:11.446175",
     "exception": false,
     "start_time": "2025-05-25T05:19:11.374353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da3bcf",
   "metadata": {
    "id": "60da3bcf",
    "papermill": {
     "duration": 0.062463,
     "end_time": "2025-05-25T05:19:11.571608",
     "exception": false,
     "start_time": "2025-05-25T05:19:11.509145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pelatihan Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab81850",
   "metadata": {
    "id": "1ab81850",
    "papermill": {
     "duration": 0.062857,
     "end_time": "2025-05-25T05:19:11.697273",
     "exception": false,
     "start_time": "2025-05-25T05:19:11.634416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Dilakukan proses pelatihan dengan data latih bersumber dari trainloader, serta data validasi bersumber dari testloader. Pada setiap epoch, dipantau akurasi, loss, serta dicatat waktu pelatihannya. Kemudian dilakukan ploting terhadap loss dan akurasi selama fase pelatihan.\n",
    "\n",
    "Referensi kode dengan modifikasi:\n",
    "\n",
    "https://www.kaggle.com/code/luckily66/gpipe-of-monocnn101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b00e64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T05:19:11.829287Z",
     "iopub.status.busy": "2025-05-25T05:19:11.828999Z",
     "iopub.status.idle": "2025-05-25T07:01:37.205280Z",
     "shell.execute_reply": "2025-05-25T07:01:37.204305Z"
    },
    "executionInfo": {
     "elapsed": 20909513,
     "status": "ok",
     "timestamp": 1748197257843,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "c4b00e64",
    "outputId": "533384f1-f7f7-4f6b-ca63-a41786d1ae53",
    "papermill": {
     "duration": 6145.445294,
     "end_time": "2025-05-25T07:01:37.206681",
     "exception": false,
     "start_time": "2025-05-25T05:19:11.761387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "start_epoch = 0\n",
    "c_type_errors = {}\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Use CUDA' if torch.cuda.is_available() else 'Use CPU')\n",
    "\n",
    "net = net.to(device)\n",
    "train_accuracy_list, val_accuracy_list, train_loss_list, val_loss_list = [],[],[],[]\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: {:d}  lr: {:.4f}'.format(epoch, scheduler.get_last_lr()[0]))\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, net.parameters()), max_norm=5)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    # Calculate and print average loss and accuracy for the epoch\n",
    "    avg_loss = train_loss / len(trainloader)\n",
    "    accuracy = 100. * correct / total\n",
    "    train_accuracy_list.append(accuracy)\n",
    "    train_loss_list.append(avg_loss)\n",
    "    print('Train Loss: {:.3f} | Acc: {:.3f}% ({:d}/{:d})'.format(avg_loss, accuracy, correct, total))\n",
    "    if torch.isnan(torch.tensor(avg_loss)):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def test(net, testloader, is_train = True):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    correct_sample_indices = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            # inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct_sample_indices.extend(predicted.eq(targets).cpu().detach().numpy().tolist())\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if total >= 10000:\n",
    "                break\n",
    "    accuracy = 100. * correct / total\n",
    "    loss = test_loss / (batch_idx + 1)\n",
    "    if(is_train):\n",
    "        val_accuracy_list.append(accuracy)\n",
    "        val_loss_list.append(loss)\n",
    "    print('Test Loss: {:.3f} | Acc: {:.3f} ({:d}/{:d})'.format(loss, accuracy, correct, total))\n",
    "    return accuracy, np.where(correct_sample_indices)[0]\n",
    "\n",
    "\n",
    "save_data_dict = OrderedDict()\n",
    "start_time = time.time()\n",
    "for epoch in range(start_epoch, start_epoch+n_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    terminate = train(epoch)\n",
    "    acc, sampler_indices = test(net, testloader)\n",
    "    scheduler.step()\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time = epoch_end_time - epoch_start_time\n",
    "    print(f'Time elapsed: %.2f s' % epoch_time)\n",
    "    if epoch == start_epoch+n_epoch-1:\n",
    "        c_type_errors['original'] = 1-acc/100\n",
    "    if terminate:\n",
    "        break\n",
    "\n",
    "save_data_dict['acc'] = acc\n",
    "save_data_dict['state_dict'] = net.state_dict()\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print('\\nFinished Training\\nTotal Training Time: %.2f seconds' % training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea0a7d",
   "metadata": {
    "id": "c1ea0a7d",
    "papermill": {
     "duration": 0.065917,
     "end_time": "2025-05-25T07:01:37.339209",
     "exception": false,
     "start_time": "2025-05-25T07:01:37.273292",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "3. Visualisasi Progress Pelatihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L2G33x_5UAjp",
   "metadata": {
    "executionInfo": {
     "elapsed": 2801,
     "status": "ok",
     "timestamp": 1748197260647,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "L2G33x_5UAjp"
   },
   "outputs": [],
   "source": [
    "# Save model ke file\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': net.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'best_acc': acc,\n",
    "    'train_accuracy_list': train_accuracy_list,\n",
    "    'val_accuracy_list': val_accuracy_list,\n",
    "    'train_loss_list': train_loss_list,\n",
    "    'val_loss_list': val_loss_list,\n",
    "}, f'{drive_path}vit_timm_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de61b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T07:01:37.475036Z",
     "iopub.status.busy": "2025-05-25T07:01:37.474726Z",
     "iopub.status.idle": "2025-05-25T07:01:37.955865Z",
     "shell.execute_reply": "2025-05-25T07:01:37.955258Z"
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1748197261173,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "22de61b5",
    "outputId": "8ba0c0c8-cec3-4507-f8ce-6ff3172b768b",
    "papermill": {
     "duration": 0.550747,
     "end_time": "2025-05-25T07:01:37.957095",
     "exception": false,
     "start_time": "2025-05-25T07:01:37.406348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot train and validation accuracy & loss\n",
    "def plot_train_val(train_list, val_list, metric_type, EPOCHS):\n",
    "    plt.plot(range(1, EPOCHS + 1), train_list, label='Train')\n",
    "    plt.plot(range(1, EPOCHS + 1), val_list, label='Validation')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_type)\n",
    "    plt.title(f'Train-Validation {metric_type}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{drive_path}train_val_{metric_type.lower()}.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_train_val(train_accuracy_list, val_accuracy_list, 'Accuracy (%)', EPOCHS)\n",
    "plot_train_val(train_loss_list, val_loss_list, 'Loss', EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a0a24",
   "metadata": {
    "id": "a69a0a24",
    "papermill": {
     "duration": 0.069299,
     "end_time": "2025-05-25T07:01:38.100023",
     "exception": false,
     "start_time": "2025-05-25T07:01:38.030724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pengujian dengan Dataset CIFAR10-Corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8336eae",
   "metadata": {
    "id": "c8336eae",
    "papermill": {
     "duration": 0.069242,
     "end_time": "2025-05-25T07:01:38.238823",
     "exception": false,
     "start_time": "2025-05-25T07:01:38.169581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "CIFAR-10-C merupakan dataset yang dibentuk oleh (Hendrycks and Dietterich, ICLR2019) untuk melakukan benchmarking kehandalan berbagai deep learning classifiers terhadap berbagai kerusakan atau noise pada gambar objek.\n",
    "\n",
    "CIFAR-10-C berasal dari sampel uji CIFAR-10 yang asli, lalu diberikan variasi dengan berbagai jenis kerusakan (corruption) dan tingkat severity. Terdapat 19 jenis keruasakan dan 5 tingkatan severity pada dataset CIFAR-10-C. Di sini digunakan data CIFAR-10-C yang diambil dari Kaggle, tetapi juga bersumber dari penelitian Hendrycks and Dietterich.\n",
    "\n",
    "Jenis kerusakannya adalah sebagai berikut:\n",
    "\n",
    "'brightness', 'contrast', 'defocus_blur', 'elastic_transform', 'fog', 'frost', 'gaussian_blur', 'gaussian_noise', 'glass_blur', 'impulse_noise', 'jpeg_compression', 'motion_blur', 'pixelate', 'saturate', 'shot_noise', 'snow', 'spatter', 'speckle_noise', 'zoom_blur'\n",
    "\n",
    "Selanjutnya dilakukan proses pengujian terhadap model yang telah dilatih sebelumnya. Pengujian menggunakan dataloader CIFAR-10-C untuk 19 kategori kerusakan dan dipantau loss serta akurasinya. Kemudian untuk semua jenis kerusakan tersebut, dilakukan ploting dan dihitung rata-rata error serta akurasinya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a10d65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:01:38.377246Z",
     "iopub.status.busy": "2025-05-25T07:01:38.376967Z",
     "iopub.status.idle": "2025-05-25T07:02:00.005367Z",
     "shell.execute_reply": "2025-05-25T07:02:00.004573Z"
    },
    "executionInfo": {
     "elapsed": 168569,
     "status": "ok",
     "timestamp": 1748197901866,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "65a10d65",
    "papermill": {
     "duration": 21.699304,
     "end_time": "2025-05-25T07:02:00.006780",
     "exception": false,
     "start_time": "2025-05-25T07:01:38.307476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Membuat class CorruptedCIFAR\n",
    "class CorruptedCIFAR(Dataset):\n",
    "    def __init__(self, root, corruption_type = None, transform = None, target_transform = None):\n",
    "        self.corrupted_data_root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.test_data = []\n",
    "        self.test_labels = []\n",
    "        if corruption_type is None:\n",
    "            for c_type in self.corruption_types:\n",
    "                self.test_data.append(np.load(os.path.join(self.corrupted_data_root, '{}.npy'.format(c_type))))\n",
    "                self.test_labels += np.load(os.path.join(self.corrupted_data_root, 'labels.npy')).tolist()\n",
    "        else:\n",
    "            self.test_data.append(np.load(os.path.join(self.corrupted_data_root, '{}.npy'.format(corruption_type))))\n",
    "            self.test_labels += np.load(os.path.join(self.corrupted_data_root, 'labels.npy')).tolist()\n",
    "        self.test_data = np.concatenate(self.test_data, axis=0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        img, target = self.test_data[index], self.test_labels[index]\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += ' Number of datapoints:{}\\n'.format(self.__len__())\n",
    "        tmp = 'test'\n",
    "        fmt_str += ' Split:{}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.corrupted_data_root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str\n",
    "\n",
    "# Pembuatan instansiasi objek dari dataset CorruptedCIFAR\n",
    "corrupt_testloaders = {}\n",
    "for c_type in corruption_types:\n",
    "    corrupt_testset = CorruptedCIFAR(root=CIFAR10_CORRUPT_PATH, corruption_type=c_type, transform=transform_test)\n",
    "    corrupt_testloader = torch.utils.data.DataLoader(\n",
    "        corrupt_testset, batch_size=100, shuffle=False, num_workers=4)\n",
    "    corrupt_testloaders[c_type] = corrupt_testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737155ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T07:02:00.143609Z",
     "iopub.status.busy": "2025-05-25T07:02:00.143351Z",
     "iopub.status.idle": "2025-05-25T07:02:18.134037Z",
     "shell.execute_reply": "2025-05-25T07:02:18.133297Z"
    },
    "executionInfo": {
     "elapsed": 24839,
     "status": "ok",
     "timestamp": 1748197926680,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "737155ad",
    "outputId": "951e314f-9d0f-4743-f762-e0ada6df1f4c",
    "papermill": {
     "duration": 18.067631,
     "end_time": "2025-05-25T07:02:18.142674",
     "exception": false,
     "start_time": "2025-05-25T07:02:00.075043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot 1 contoh kelas CIFAR10 untuk 19 jenis corrupted image\n",
    "num_images = len(corrupt_testloaders)\n",
    "num_rows = math.ceil(num_images / 6)\n",
    "num_cols = min(num_images, 6)\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 8))\n",
    "for i, (c_type, testloader) in enumerate(corrupt_testloaders.items()):\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    img, _ = next(iter(testloader))\n",
    "\n",
    "    axes[row, col].imshow(img[0].permute(1, 2, 0))  # Permute dimensi gambar\n",
    "    axes[row, col].axis('off')\n",
    "    axes[row, col].set_title(c_type, size=11)\n",
    "\n",
    "# Menyembunyikan subplot kosong\n",
    "for i in range(num_rows * num_cols):\n",
    "    if i >= num_images:\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        axes[row, col].set_visible(False)\n",
    "\n",
    "plt.suptitle(f'Plot CIFAR10-C for Cat')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c13a10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T07:02:18.306163Z",
     "iopub.status.busy": "2025-05-25T07:02:18.305828Z",
     "iopub.status.idle": "2025-05-25T07:08:47.170689Z",
     "shell.execute_reply": "2025-05-25T07:08:47.169687Z"
    },
    "executionInfo": {
     "elapsed": 1328667,
     "status": "ok",
     "timestamp": 1748199255356,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "d9c13a10",
    "outputId": "c2e1bcbb-5dfa-469f-dd36-24ee504be87a",
    "papermill": {
     "duration": 388.946859,
     "end_time": "2025-05-25T07:08:47.172238",
     "exception": false,
     "start_time": "2025-05-25T07:02:18.225379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Melakukan evaluasi terhadap CIFAR10 Corrupted\n",
    "print(\"Evaluating on corrupted CIFAR-10 testset\\n\")\n",
    "for c_type in corruption_types:\n",
    "    print('- corruption type: {}'.format(c_type))\n",
    "    corrupt_acc, _ = test(net, corrupt_testloaders[c_type])\n",
    "    c_type_errors[c_type] = 1-corrupt_acc/100\n",
    "    save_data_dict['{}_acc'.format(c_type)] = corrupt_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b3d3d",
   "metadata": {
    "id": "1a2b3d3d",
    "papermill": {
     "duration": 0.076748,
     "end_time": "2025-05-25T07:08:47.328008",
     "exception": false,
     "start_time": "2025-05-25T07:08:47.251260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "4. Penyajian/visualisasi hasil evaluasi (accuracy atau error) keseluruhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16b47e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T07:08:47.487573Z",
     "iopub.status.busy": "2025-05-25T07:08:47.487263Z",
     "iopub.status.idle": "2025-05-25T07:08:48.083533Z",
     "shell.execute_reply": "2025-05-25T07:08:48.082873Z"
    },
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1748199256261,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "f16b47e6",
    "outputId": "7d6a0e88-b8e7-4e51-f031-13780b366c4a",
    "papermill": {
     "duration": 0.680582,
     "end_time": "2025-05-25T07:08:48.085542",
     "exception": false,
     "start_time": "2025-05-25T07:08:47.404960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot akurasi & error pengujian\n",
    "def plot_metrics(metrics, metric_type):\n",
    "    mean_metric = np.mean(list(metrics.values()))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot metrics\n",
    "    c_types = [c_type.lower() for c_type in metrics.keys()]\n",
    "    values = list(metrics.values())\n",
    "\n",
    "    if metric_type == 'error':\n",
    "        color = 'lightcoral'\n",
    "        ylabel = 'Error Rate'\n",
    "        title = 'Error Rate for Each Corruption Type\\n(Mean Error Rate = {:.2f}%)'.format(mean_metric*100)\n",
    "    elif metric_type == 'accuracy':\n",
    "        color = 'lightgreen'\n",
    "        ylabel = 'Accuracy'\n",
    "        title = 'Accuracy for Each Corruption Type\\n(Mean Accuracy = {:.2f}%)'.format(mean_metric*100)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric_type. Please use 'error' or 'accuracy'.\")\n",
    "    colors = [color if key != 'original' else 'orange' for key in c_types]\n",
    "    bars = plt.bar(c_types, values, color=colors)\n",
    "    for bar, rate in zip(bars, values):\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2, 0.05, '{:.2f}%'.format(rate*100), ha='center', color='black', fontsize=9, rotation=90)\n",
    "    plt.title(title, size=13)\n",
    "    plt.xlabel('Corruption Type', size=11)\n",
    "    plt.ylabel(ylabel, size=11)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{drive_path}{metric_type.lower()}_rate.png')\n",
    "    plt.show()\n",
    "c_type_acc = {c_type: 1 - error_rate for c_type, error_rate in c_type_errors.items()}\n",
    "plot_metrics(c_type_errors, 'error')\n",
    "plot_metrics(c_type_acc, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0983f854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T07:08:48.255337Z",
     "iopub.status.busy": "2025-05-25T07:08:48.255054Z",
     "iopub.status.idle": "2025-05-25T07:08:48.258603Z",
     "shell.execute_reply": "2025-05-25T07:08:48.258076Z"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1748199256315,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "0983f854",
    "papermill": {
     "duration": 0.086905,
     "end_time": "2025-05-25T07:08:48.259720",
     "exception": false,
     "start_time": "2025-05-25T07:08:48.172815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 10000\n",
    "SEVERITY = 5\n",
    "DATADIR = \"data\"\n",
    "SHUFFLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677cb5a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 820
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T07:08:48.422614Z",
     "iopub.status.busy": "2025-05-25T07:08:48.421828Z",
     "iopub.status.idle": "2025-05-25T07:12:56.784273Z",
     "shell.execute_reply": "2025-05-25T07:12:56.780891Z"
    },
    "executionInfo": {
     "elapsed": 183826,
     "status": "ok",
     "timestamp": 1748199440144,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "677cb5a4",
    "outputId": "7ab60e8b-7468-4ae3-a411-061250d4a658",
    "papermill": {
     "duration": 248.448816,
     "end_time": "2025-05-25T07:12:56.788093",
     "exception": false,
     "start_time": "2025-05-25T07:08:48.339277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_cifar10_c():\n",
    "    x, y = load_cifar10c(\n",
    "        len(corruption_types),\n",
    "        SEVERITY, DATADIR, SHUFFLE,\n",
    "        corruption_types\n",
    "    )\n",
    "    x_np = x.numpy()\n",
    "    x_np = np.transpose(x_np, (0, 2, 3, 1))  # Transpose to (batch_size, height, width, channels)\n",
    "    # Plot CIFAR-10-C images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(corruption_types)):  # Plot the first 25 images\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.imshow(x_np[i])\n",
    "        plt.title(f\"{corruption_types[i]}\")  # Show label\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f'Plot CIFAR10-C for Cat')\n",
    "    plt.show()\n",
    "\n",
    "plot_cifar10_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GBfw8wg6z6Uj",
   "metadata": {
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1748199695282,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "GBfw8wg6z6Uj"
   },
   "outputs": [],
   "source": [
    "del net\n",
    "del optimizer\n",
    "del criterion\n",
    "\n",
    "# Langkah 2: Bersihkan cache CUDA\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Langkah 3 (opsional tapi lebih agresif): Koleksi sampah dan clear\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GbON-Y9E0VoH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2594,
     "status": "ok",
     "timestamp": 1748199704594,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "GbON-Y9E0VoH",
    "outputId": "c54676cc-5fea-407a-cec8-ad663207de02"
   },
   "outputs": [],
   "source": [
    "# Buat ulang arsitektur model\n",
    "print('==> Building model..')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Use CUDA' if torch.cuda.is_available() else 'Use CPU')\n",
    "\n",
    "net = custom_model('vit_timm')\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(f'{drive_path}vit_timm_best_model.pth', map_location=device)\n",
    "net.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "start_epoch = checkpoint['epoch']\n",
    "best_acc = checkpoint['best_acc']\n",
    "print(f\"Model loaded, last accuracy: {best_acc:.2f}% at epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26418b0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T07:12:57.059295Z",
     "iopub.status.busy": "2025-05-25T07:12:57.058656Z",
     "iopub.status.idle": "2025-05-25T07:22:20.777000Z",
     "shell.execute_reply": "2025-05-25T07:22:20.776198Z"
    },
    "executionInfo": {
     "elapsed": 1816713,
     "status": "ok",
     "timestamp": 1748201525921,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "26418b0f",
    "outputId": "16adc52b-f8b4-407a-9354-b3f07f6a5f21",
    "papermill": {
     "duration": 563.968311,
     "end_time": "2025-05-25T07:22:20.895476",
     "exception": false,
     "start_time": "2025-05-25T07:12:56.927165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corruption_errors = {}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert to PIL Image\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Define batch size for testing\n",
    "test_batch_size = 16  # Adjust this value as needed\n",
    "\n",
    "# Iterate over corrupted images one corruption at a time\n",
    "for corruption_type in corruption_types:\n",
    "    print(\"Corruption type: \", corruption_type)\n",
    "    images, labels = load_cifar10c(NUM_EXAMPLES, SEVERITY, DATADIR, SHUFFLE, [corruption_type])\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # Process images in batches\n",
    "    for i in range(0, len(images), test_batch_size):\n",
    "        batch_images = images[i:i+test_batch_size]\n",
    "        batch_labels = labels[i:i+test_batch_size]\n",
    "\n",
    "        batch_images = torch.stack([transform(image) for image in batch_images])\n",
    "        batch_images = batch_images.to(device).float()\n",
    "        batch_labels = batch_labels.to(device).float()\n",
    "\n",
    "        outputs = net(batch_images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    corruption_errors[corruption_type] = 1-acc/100\n",
    "    print(f'Accuracy of the network on {corruption_type} images: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d430c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-05-25T07:22:21.128458Z",
     "iopub.status.busy": "2025-05-25T07:22:21.127762Z",
     "iopub.status.idle": "2025-05-25T07:22:21.688883Z",
     "shell.execute_reply": "2025-05-25T07:22:21.688143Z"
    },
    "executionInfo": {
     "elapsed": 980,
     "status": "ok",
     "timestamp": 1748201572493,
     "user": {
      "displayName": "23523034 Husni Fadhilah Dhiya Ul Haq",
      "userId": "06215843376475841561"
     },
     "user_tz": -420
    },
    "id": "f19d430c",
    "outputId": "9aa78a0c-1e43-454d-cb8e-bb1912653114",
    "papermill": {
     "duration": 0.679491,
     "end_time": "2025-05-25T07:22:21.690269",
     "exception": false,
     "start_time": "2025-05-25T07:22:21.010778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corruption_errors['original'] = c_type_errors['original']\n",
    "# del corruption_errors['original']\n",
    "corruption_acc = {c_type: 1 - error_rate for c_type, error_rate in corruption_errors.items()}\n",
    "plot_metrics(corruption_errors, 'error')\n",
    "plot_metrics(corruption_acc, 'accuracy')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2902123,
     "sourceId": 5002176,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3859521,
     "sourceId": 6694150,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7122663,
     "sourceId": 11376681,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7520.578749,
   "end_time": "2025-05-25T07:22:25.058410",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-25T05:17:04.479661",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0158009f6d4d495a82a208cd60737590": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33bba534724e4342b298859d766d1b6a",
      "placeholder": "",
      "style": "IPY_MODEL_6f0af9e8a2794d2ebfb7fc00825b7cdc",
      "value": "model.safetensors:100%"
     }
    },
    "33bba534724e4342b298859d766d1b6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d22d8f72880429eab807577d3510b6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f0af9e8a2794d2ebfb7fc00825b7cdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71817e0194f14378b533cbb1c72c2987": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "77544320956d43c98a3945dbdde234b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d22d8f72880429eab807577d3510b6f",
      "max": 347452074,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_71817e0194f14378b533cbb1c72c2987",
      "value": 347452074
     }
    },
    "79e536af7f3f447e8fb86630e08cd24e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bc0a6b6b103e4a3e9f66dad6e165a86f",
      "placeholder": "",
      "style": "IPY_MODEL_addb4c4b2679449596aa2eb4a38dcf1f",
      "value": "347M/347M[00:00&lt;00:00,424MB/s]"
     }
    },
    "88c365a17851409eaff0b8ddf582b45f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cd21ab96eef4b54b4a1acc0938e8150": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0158009f6d4d495a82a208cd60737590",
       "IPY_MODEL_77544320956d43c98a3945dbdde234b3",
       "IPY_MODEL_79e536af7f3f447e8fb86630e08cd24e"
      ],
      "layout": "IPY_MODEL_88c365a17851409eaff0b8ddf582b45f"
     }
    },
    "addb4c4b2679449596aa2eb4a38dcf1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc0a6b6b103e4a3e9f66dad6e165a86f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
